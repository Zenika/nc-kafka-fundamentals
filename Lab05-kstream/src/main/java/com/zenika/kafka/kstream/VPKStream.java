package com.zenika.kafka.kstream;

import com.zenika.kafka.common.model.LightPositionValue;
import com.zenika.kafka.common.model.PositionKey;
import com.zenika.kafka.common.model.PositionValue;
import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import java.util.regex.Pattern;

import static io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;

@Component
@Slf4j
public class VPKStream {

    @Value("${application.kafka.kstream.bootstrap-servers}")
    private String bootstrapServer;
    @Value("${application.kafka.kstream.schema.registry.url}")
    private String schemaRegistryUrl;
    @Value("${application.kafka.kstream.client-id}")
    private String clientId;
    @Value("${application.kafka.kstream.application-id}")
    private String applicationId;
    @Value("${application.input-topic}")
    private String inputTopic;
    @Value("${application.output-filtered-topic}")
    private String outputFilteredTopic;
    @Value("${application.output-light-topic}")
    private String outputLightTopic;

    private KafkaStreams filterKStreams;
    private KafkaStreams mappingKStreams;

    @PostConstruct
    public void startFilterStream() {

        log.info("Starting Filtering Kafka Streams");

        final StreamsBuilder builder = new StreamsBuilder();

        // read avro partition
        final KStream<PositionKey, PositionValue> avroToAvroStream = builder.stream(
                Pattern.compile(inputTopic),
                Consumed.with(SerdesBuilder.build(schemaRegistryUrl, true),
                        SerdesBuilder.build(schemaRegistryUrl, false))
        );

        // Apply filter operation on oper (22 Nobina Finland Oy) citeria
        KStream<PositionKey, PositionValue> filter = avroToAvroStream.filter(
                (readOnlyKey, value) -> value.getOper() == 22
        );

        // Output data
        filter.to(outputFilteredTopic, Produced.with(SerdesBuilder.build(schemaRegistryUrl, true),
                SerdesBuilder.build(schemaRegistryUrl, false))
        );

        // print topology
        printTopology(builder.build());

        filterKStreams = new KafkaStreams(builder.build(), buildProperties("filter"));
        filterKStreams.start();

        log.info("Kafka Filtering Streams is started.");
    }

    @PostConstruct
    public void startLightMappingStream() {
        log.info("Starting Light Mapping Kafka Streams");

        final StreamsBuilder builder = new StreamsBuilder();

        // read avro partition
        final KStream<PositionKey, PositionValue> avroToAvroStream = builder.stream(
                Pattern.compile(inputTopic),
                Consumed.with(SerdesBuilder.build(schemaRegistryUrl, true),
                        SerdesBuilder.build(schemaRegistryUrl, false)
                )
        );

        // Apply mapping to LightPositionValue
        KStream<PositionKey, LightPositionValue> filter = avroToAvroStream.mapValues(
                (readOnlyKey, value) -> LightPositionValue.newBuilder()
                        .setLine(value.getLine())
                        .setOper(value.getOper())
                        .setLat(value.getLat())
                        .setLong$(value.getLong$())
                        .build()
        );

        // Output data
        filter.to(outputLightTopic, Produced.with(SerdesBuilder.build(schemaRegistryUrl, true),
                SerdesBuilder.build(schemaRegistryUrl, false))
        );

        // print topology
        printTopology(builder.build());

        mappingKStreams = new KafkaStreams(builder.build(), buildProperties("mapping"));
        mappingKStreams.start();

        log.info("Kafka Light Mapping Streams is started.");
    }


    // 3 - last data for all oper with compaction topic (bonus)
    // create ktable with last positions
    // expose KTable with REst

    @PreDestroy
    public void onStop() {
        log.info("Stopping Kafka Streams...");
        filterKStreams.close();
        mappingKStreams.close();
    }

    private Properties buildProperties(String streamName) {
        // config
        final Properties configs = new Properties();

        // Kafka connection configuration
        configs.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer);

        // SerDes configuration
        configs.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, SpecificAvroSerde.class);
        configs.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class);

        // Schema registry configuration
        configs.put(SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl);

        // Is equivalent of client.id for consumer & producer
        configs.put(StreamsConfig.CLIENT_ID_CONFIG, String.format("%s-%s", streamName, clientId));

        // Is equivalent of group.id for consumer & producer
        configs.put(StreamsConfig.APPLICATION_ID_CONFIG, String.format("%s-%s", streamName, applicationId));

        // This setting helps to identify the offset from where the consumer starts reading the data. Here are the options :
        //  * earliest: automatically reset the offset to the earliest offset
        //  * latest: automatically reset the offset to the latest offset
        //  * none: throw exception to the consumer if no previous offset is found for the consumerâ€™s group
        //  * anything else: throw exception to the consumer.
        configs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        return configs;
    }

    private void printTopology(Topology topology) {
        log.info(topology.describe().toString());
    }
}
