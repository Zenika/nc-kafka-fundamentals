# Lab04 - Schema Registry

## Rappel

<p style="text-align:center">
<img src="lab04.png" alt="lab04" />
</p>


<p style="text-align:center">
<img src="lab04.bis.png" alt="lab04-bis" />
</p>

## Le Schema Registry de Confluent

Schema Registry est une couche de stockage distribu√©e pour les sch√©mas qui utilise Kafka comme m√©canisme de stockage
sous-jacent.
![schema-registry-and-kafka.png](schema-registry-and-kafka.png)

Schema Registry est sous licence Confluent Community License.

Le repository Github: [https://github.com/confluentinc/schema-registry](https://github.com/confluentinc/schema-registry)
.

La documentation officielle de
confluent: [https://docs.confluent.io/platform/current/schema-registry/index.html](https://docs.confluent.io/platform/current/schema-registry/index.html)
.

## Pr√©parer le projet et le topic Kafka

- ‚ö†Ô∏è Checkout de la branche `step04` ‚ö†Ô∏è.

- Se placer dans le repertoire `Lab04-avro`

- Demarrer le conteneur Schema registry.

```bash
docker-compose -f docker-compose-schema.yml up -d
```

### Le topic Kafka

- Au sein de ce lab nous utilisons [spring-kafka](https://spring.io/projects/spring-kafka) pour dialoguer avec Kafka au
  sein de l'√©cosyst√®me Spring Boot.

- Spring Kafka fourni des composants qui permettent de vous "faciliter la vie", il permet de facilement cr√©√©r des topics
  au sein de votre application:
  [https://docs.spring.io/spring-kafka/reference/html/#configuring-topics](https://docs.spring.io/spring-kafka/reference/html/#configuring-topics)

- Le bean `NewTopic` provoque la cr√©ation du topic sur le broker; il n'est pas n√©cessaire si le topic existe d√©j√†.

```java
@Bean
NewTopic schemaAvroTopic(){
    return TopicBuilder.name(topic).partitions(1).replicas(1).build();
}
```

- ‚ö†Ô∏è Spring Kafka utilise l'API admin du cluster pour r√©aliser cette op√©ration de cr√©ation de topic. M√™me si vous
  specifier `KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'`, le param√©trage d'un bean `KafkaAdmin` permet de manipuler l'API
  admin.

## Un peu de code

- Avro a un mod√®le de donn√©es de type JSON, mais peut √™tre repr√©sent√© en tant que JSON ou sous une forme binaire
  compacte. Il est livr√© avec un langage de description de sch√©ma tr√®s sophistiqu√© qui d√©crit les donn√©es.
    1. Direct Mapping vers et depuis JSON
    2. Il dispose d'un format tr√®s compact.
    3. C'est tr√®s rapide.
    4. Vous pouvez g√©n√©rer des objets Java pour des √©v√©nements Kafka, mais il ne n√©cessite pas de g√©n√©ration de code
       afin que les outils puissent √™tre √©crits de mani√®re g√©n√©rique pour tout flux de donn√©es.
    5. Il dispose d'un langage de sch√©ma riche et extensible d√©fini en JSON.
    6. Il dispose de la meilleure notion de compatibilit√© pour faire √©voluer vos donn√©es/sch√©mas dans le temps.

- Pour plus d'informations sur l'usage
  d'Avro: [https://www.confluent.fr/blog/avro-kafka-data/](https://www.confluent.fr/blog/avro-kafka-data/)

- Explorez le d√©coupage projet
    * `consumer`
    * `producer`
    * `src/main/avro`

- Au sein de ce projet `Lab04-avro` nous utilisons un plugin maven nous permettant de g√©n√©rer nos entit√©s m√©tiers
  d'√©v√©nements Kafka.

```xml

<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.10.2</version>
    <executions>
        <execution>
            <phase>process-sources</phase>
            <goals>
                <goal>schema</goal>
            </goals>
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/avro/</sourceDirectory>
                <outputDirectory>${project.basedir}/target/generated-sources/avro</outputDirectory>
            </configuration>
        </execution>
    </executions>
</plugin>
```

- Pour g√©n√©rer les entit√©s, il est n√©cessaire d'√©xecuter la phase maven `compile` qui appliquera le `process-sources`.

> Se placer dans le bon r√©pertoire `Lab04-avro`

```bash
./mvnw clean compile`
```

- Observez les POJO g√©n√©r√©s cf. `target/generated-sources/avro`

- Maintenant vous pouvez utiliser directement des objets java dans votre code, pas besoin de passer par un `String`,
  vous pouvez exploiter des objets complexes.
  
### Partie `Producer`

> La classe VehiclePositionProducer.java

```java
private final ProducerFactory<PositionKey, PositionValue> producerFactory;
```

- Et surtout param√©trer des cl√©s complexes pour vos `Record`:

> La classe Subscriber.java

```java
final PositionValue value = getPositionValue(message.getPayload());
final PositionKey key = new PositionKey(topic);
final ProducerRecord<PositionKey, PositionValue> record = new ProducerRecord<>(kafkaTopic,key,value);
producer.send(record);
```

### Partie `Consummer`

> La classe KafkaRestConsumer.java
```java
Consumer<PositionKey, PositionValue> consumer = kafkaConsumerFactory.createConsumer();
//...
ConsumerRecords<PositionKey, PositionValue> records = consumer.poll(Duration.ofSeconds(5));
```


### D√©marrer votre application en local

-Il s'agit d'un projet Maven qui dispose d'un wrapper `mvnw` et du plugin `spring-boot-maven-plugin`, vous pouvez
d√©marrer votre application spring en local √† l'aide de la commande suivante:

> Se placer dans le bon r√©pertoire `Lab04-avro`

```shell
./mvnw spring-boot:run
```

- Visualiser la consommation des messages sur l'url suivante: [http://localhost:8092](http://localhost:8092)

> * Que voyez-vous sur l'IHM ?
> * Observez les schemas AKHQ [http://akhq:8080/](http://localhost:8080/ui/server/schema) ?
> * Ou sont stock√©s les sch√©mas ?

## Packager votre application avec Docker

- Pour builder et d√©marrer le conteneur

> Se placer dans le bon r√©pertoire `Lab04-avro`

```bash
docker build -t vp-avro-producer-consumer .
```

```bash
docker run --name vp-avro-producer-consumer --network=tz-kafka-network -p 8092:8092 -d vp-avro-producer-consumer
```

## Solution

Vous vous doutez que pour disposer des solutions de la `step04`, il vous suffit deÔ∏è checkout la branche `step05` üòä